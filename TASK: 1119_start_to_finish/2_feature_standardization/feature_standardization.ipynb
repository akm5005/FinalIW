{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code loads and preprocesses data by separating training data (2010–2016), removing non-numeric columns, replacing '1,000+' with '1000', and handling zeros with a small constant to prepare for transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Paths to the data and metadata files\n",
    "data_path = \"/Users/avakrocheski-meyer/Downloads/TASK: 1119_start_to_finish/1_feature_filtering/final_data.csv\"\n",
    "metadata_path = \"/Users/avakrocheski-meyer/Downloads/TASK: 1119_start_to_finish/1_feature_filtering/final_metadata.csv\"\n",
    "output_data_path = \"/Users/avakrocheski-meyer/Downloads/TASK: 1119_start_to_finish/2_feature_standardization/final_data_transformed.csv\"\n",
    "output_metadata_path = \"/Users/avakrocheski-meyer/Downloads/TASK: 1119_start_to_finish/2_feature_standardization/final_metadata_with_transformation.csv\"\n",
    "\n",
    "# Load data and metadata\n",
    "df = pd.read_csv(data_path)\n",
    "metadata_df = pd.read_csv(metadata_path)\n",
    "\n",
    "# Separate the training data (2010–2016)\n",
    "train_df = df[df['Year'] <= 2016]\n",
    "\n",
    "# Drop 'GEO_ID' and 'Year' columns for transformations\n",
    "train_df = train_df.drop(columns=['GEO_ID', 'Year'])\n",
    "\n",
    "# Replace any values that are '1,000+' with '1000' and convert all columns to numeric\n",
    "train_df.replace({r'1,000\\+': '1000'}, regex=True, inplace=True)\n",
    "df.replace({r'1,000\\+': '1000'}, regex=True, inplace=True)\n",
    "# train_df = train_df.apply(pd.to_numeric, errors='coerce')  # Coerce non-numeric to NaN\n",
    "\n",
    "# Replace all zeros in the training DataFrame with a small positive constant\n",
    "train_df.replace(0, 1e-9, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code determines appropriate transformations for each column based on skewness using training data, updates metadata with transformation details, and applies the transformations (logarithmic, standardization, or both) consistently to the full dataset before saving the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated metadata with transformation column saved to: /Users/avakrocheski-meyer/Downloads/TASK: 1119_start_to_finish/2_feature_standardization/final_metadata_with_transformation.csv\n",
      "Transformed dataset saved to: /Users/avakrocheski-meyer/Downloads/TASK: 1119_start_to_finish/2_feature_standardization/final_data_transformed.csv\n"
     ]
    }
   ],
   "source": [
    "# Define function to determine transformations based on skewness and standard deviation\n",
    "def determine_transformations(df, threshold_skew=1):\n",
    "    transformations = {}\n",
    "    for column in df.columns:\n",
    "        # Skip non-numeric columns and explicitly exclude target_TOT_POPULATION\n",
    "        if not np.issubdtype(df[column].dtype, np.number):\n",
    "            continue\n",
    "        if column == 'target_TOT_POPULATION':\n",
    "            transformations[column] = \"log only\"\n",
    "            continue\n",
    "\n",
    "        # Calculate skewness and standard deviation\n",
    "        skewness = df[column].skew()\n",
    "        \n",
    "        # Determine transformation based on skewness and standard deviation\n",
    "        if abs(skewness) > threshold_skew:\n",
    "            transformations[column] = \"log + standardize\"\n",
    "        else:\n",
    "            transformations[column] = \"standardize only\"\n",
    "    \n",
    "    return transformations\n",
    "\n",
    "# Determine transformations using the training data\n",
    "transformations = determine_transformations(train_df)\n",
    "metadata_df['transformation'] = metadata_df['variable'].map(transformations).fillna(\"standardize only\")\n",
    "\n",
    "# Save the updated metadata file with transformation details\n",
    "metadata_df.to_csv(output_metadata_path, index=False)\n",
    "print(\"Updated metadata with transformation column saved to:\", output_metadata_path)\n",
    "\n",
    "# Apply the determined transformations using training statistics\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for column, transformation in transformations.items():\n",
    "    if transformation == \"log + standardize\":\n",
    "        # Apply log transformation and then standardization using training parameters\n",
    "        train_df[column] = np.log(train_df[column].replace(0, 1e-9))\n",
    "        scaler.fit(train_df[[column]])  # Fit scaler on training data\n",
    "        df[column] = np.log(df[column].replace(0, 1e-9))  # Apply log transformation to full dataset\n",
    "        df[column] = scaler.transform(df[[column]])  # Transform full dataset using training parameters\n",
    "    elif transformation == \"standardize only\":\n",
    "        # Standardize only using training parameters\n",
    "        scaler.fit(train_df[[column]])  # Fit scaler on training data\n",
    "        df[column] = scaler.transform(df[[column]])  # Transform full dataset using training parameters\n",
    "    elif transformation == \"log only\":\n",
    "        df[column] = df[column].replace(0, 1e-9)  # Replace 0s to avoid log(0)\n",
    "        df[column] = np.where(df[column].notna(), np.log(df[column]), np.nan)  # Apply log only to non-NaN values\n",
    "\n",
    "# Save the fully transformed dataset\n",
    "df.to_csv(output_data_path, index=False)\n",
    "print(\"Transformed dataset saved to:\", output_data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code checks for invalid values (NaNs and infinities) in a dataset, excluding the GEO_ID column, and reports columns containing such values, ensuring that only the target_TOT_POPULATION column is expected to have NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_TOT_POPULATION should have NaNs; nothing else\n",
      "Columns with invalid values:\n",
      "Column 'target_TOT_POPULATION': {'NaNs': 101, 'Infinities': 0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to identify columns with invalid values (NaNs and infinities)\n",
    "def find_invalid_values(df):\n",
    "    invalid_values_report = {}\n",
    "\n",
    "    for column in df.columns:\n",
    "        # Convert the column to numeric to ensure np.isinf works (non-numeric values will become NaN)\n",
    "        numeric_col = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "        # Check for NaNs\n",
    "        nan_count = numeric_col.isnull().sum()\n",
    "\n",
    "        # Check for infinities\n",
    "        inf_count = np.isinf(numeric_col).sum()\n",
    "\n",
    "        \n",
    "        # Store details if any invalid values are found\n",
    "        if nan_count > 0 or inf_count > 0:\n",
    "            invalid_values_report[column] = {\n",
    "                \"NaNs\": nan_count,\n",
    "                \"Infinities\": inf_count\n",
    "            }\n",
    "\n",
    "    return invalid_values_report\n",
    "\n",
    "\n",
    "# Create a copy of df without the GEO_ID column\n",
    "df_without_geo_id = df.drop(columns=['GEO_ID'])\n",
    "\n",
    "# Run the function and display results\n",
    "invalid_values_report = find_invalid_values(df_without_geo_id)\n",
    "\n",
    "print(\"target_TOT_POPULATION should have NaNs; nothing else\")\n",
    "\n",
    "# Display any columns with invalid values\n",
    "if invalid_values_report:\n",
    "    print(\"Columns with invalid values:\")\n",
    "    for column, issues in invalid_values_report.items():\n",
    "        print(f\"Column '{column}': {issues}\")\n",
    "else:\n",
    "    print(\"No invalid values found in the dataset.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
