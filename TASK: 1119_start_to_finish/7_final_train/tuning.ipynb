{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code loads a dataset, splits it into training (2010–2016) and test sets (2017–2019), separates the features and target variable for both sets, and confirms their shapes to ensure the data is correctly prepared for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_features shape: (707, 150)\n",
      "y_train shape: (707,)\n",
      "X_test_features shape: (202, 150)\n",
      "y_test shape: (202,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('/Users/avakrocheski-meyer/Downloads/TASK: 1119_start_to_finish/6_feature_generation/all_lagged.csv') \n",
    "\n",
    "# Define the training and test data years\n",
    "train_years = range(2010, 2016 + 1)  # Training data from 2010 to 2016\n",
    "test_years = range(2017, 2019)   # Test data from 2017 to 2018\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "train_df = df[df['Year'].isin(train_years)].copy()\n",
    "test_df = df[df['Year'].isin(test_years)].copy()\n",
    "\n",
    "# Define the target column\n",
    "target_column = 'target_TOT_POPULATION'  # Replace with your actual target column name\n",
    "\n",
    "# Define feature columns to drop\n",
    "columns_to_drop = ['GEO_ID', 'Year', target_column]\n",
    "\n",
    "# Separate features and target for training\n",
    "X_train_features = train_df.drop(columns=columns_to_drop)\n",
    "y_train = train_df[target_column]\n",
    "\n",
    "# Separate features and target for testing\n",
    "X_test_features = test_df.drop(columns=columns_to_drop)\n",
    "y_test = test_df[target_column]\n",
    "\n",
    "# Confirm the shapes of the datasets\n",
    "print(\"X_train_features shape:\", X_train_features.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test_features shape:\", X_test_features.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code implements expanding window cross-validation, where a model is trained on incrementally larger training sets and evaluated on the next year's data, with evaluation metrics (MSE, MAE, RMSE) computed for each validation iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Function to perform cross-validation and return evaluation metrics\n",
    "def cross_val_model(X_train, y_train, X_val, y_val, model):\n",
    "    #validate_data(X_train, y_train)\n",
    "    #validate_data(X_val, y_val)\n",
    "\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Convert back from log scale\n",
    "    # y_pred_exp = np.exp(y_pred)\n",
    "    # y_val_exp = np.exp(y_val)\n",
    "    \n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return mse, mae, rmse\n",
    "\n",
    "\n",
    "# Function to perform expanding window cross-validation\n",
    "def expanding_window_cv(df, start_year, end_year, target_column, model):\n",
    "    results = []\n",
    "    years = sorted(df['Year'].unique())\n",
    "    years = [y for y in years if start_year <= y <= end_year]\n",
    "    for i in range(3, len(years)):\n",
    "        train_years = years[:i]\n",
    "        val_year = years[i]\n",
    "        train_data = df[df['Year'].isin(train_years)]\n",
    "        val_data = df[df['Year'] == val_year]\n",
    "\n",
    "        # Separate features and target\n",
    "        X_train = train_data.drop(columns=[target_column, 'GEO_ID', 'Year'])\n",
    "        y_train = train_data[target_column]\n",
    "        X_val = val_data.drop(columns=[target_column, 'GEO_ID', 'Year'])\n",
    "        y_val = val_data[target_column]\n",
    "\n",
    "\n",
    "        # Train and evaluate model\n",
    "        mse, mae, rmse = cross_val_model(X_train, y_train, X_val, y_val, model)\n",
    "        results.append({'Year': val_year, 'MSE': mse, 'MAE': mae, 'RMSE': rmse})\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs hyperparameter tuning for a Random Forest model using a grid search over specified parameter combinations and evaluates each configuration with expanding window cross-validation, identifying the best parameters based on the lowest mean MSE across validation segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: {'n_estimators': 50, 'max_depth': None, 'min_samples_split': 2} Mean MSE: 0.00034903714636115254\n",
      "Params: {'n_estimators': 50, 'max_depth': None, 'min_samples_split': 5} Mean MSE: 0.000535969370298172\n",
      "Params: {'n_estimators': 50, 'max_depth': None, 'min_samples_split': 10} Mean MSE: 0.0022264747777607586\n",
      "Params: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 2} Mean MSE: 0.00031322622198988253\n",
      "Params: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 5} Mean MSE: 0.0005097768586732026\n",
      "Params: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 10} Mean MSE: 0.0022332656955308715\n",
      "Params: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 2} Mean MSE: 0.00034903714636115146\n",
      "Params: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 5} Mean MSE: 0.0005359693702981668\n",
      "Params: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 10} Mean MSE: 0.0022264747777607495\n",
      "Params: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 2} Mean MSE: 0.0003490371463611508\n",
      "Params: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 5} Mean MSE: 0.0005359693702981675\n",
      "Params: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 10} Mean MSE: 0.002226474777760758\n",
      "Params: {'n_estimators': 100, 'max_depth': None, 'min_samples_split': 2} Mean MSE: 0.0003160976271809775\n",
      "Params: {'n_estimators': 100, 'max_depth': None, 'min_samples_split': 5} Mean MSE: 0.0005272175906880755\n",
      "Params: {'n_estimators': 100, 'max_depth': None, 'min_samples_split': 10} Mean MSE: 0.002110889164679244\n",
      "Params: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2} Mean MSE: 0.00028139270542493135\n",
      "Params: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 5} Mean MSE: 0.0005053691342808081\n",
      "Params: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 10} Mean MSE: 0.002111869361645701\n",
      "Params: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 2} Mean MSE: 0.0003160976271809787\n",
      "Params: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 5} Mean MSE: 0.0005272175906880764\n",
      "Params: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 10} Mean MSE: 0.002110889164679247\n",
      "Params: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2} Mean MSE: 0.0003160976271809756\n",
      "Params: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5} Mean MSE: 0.0005272175906880892\n",
      "Params: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 10} Mean MSE: 0.0021108891646792315\n",
      "Params: {'n_estimators': 150, 'max_depth': None, 'min_samples_split': 2} Mean MSE: 0.0003222898412071046\n",
      "Params: {'n_estimators': 150, 'max_depth': None, 'min_samples_split': 5} Mean MSE: 0.00047371105086506\n",
      "Params: {'n_estimators': 150, 'max_depth': None, 'min_samples_split': 10} Mean MSE: 0.002163683203023157\n",
      "Params: {'n_estimators': 150, 'max_depth': 10, 'min_samples_split': 2} Mean MSE: 0.00027088243348408897\n",
      "Params: {'n_estimators': 150, 'max_depth': 10, 'min_samples_split': 5} Mean MSE: 0.0004626651485315003\n",
      "Params: {'n_estimators': 150, 'max_depth': 10, 'min_samples_split': 10} Mean MSE: 0.0021800842469666356\n",
      "Params: {'n_estimators': 150, 'max_depth': 20, 'min_samples_split': 2} Mean MSE: 0.000322289841207104\n",
      "Params: {'n_estimators': 150, 'max_depth': 20, 'min_samples_split': 5} Mean MSE: 0.00047371105086505374\n",
      "Params: {'n_estimators': 150, 'max_depth': 20, 'min_samples_split': 10} Mean MSE: 0.002163683203023153\n",
      "Params: {'n_estimators': 150, 'max_depth': 30, 'min_samples_split': 2} Mean MSE: 0.0003222898412070984\n",
      "Params: {'n_estimators': 150, 'max_depth': 30, 'min_samples_split': 5} Mean MSE: 0.0004737110508650494\n",
      "Params: {'n_estimators': 150, 'max_depth': 30, 'min_samples_split': 10} Mean MSE: 0.0021636832030231447\n",
      "Params: {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 2} Mean MSE: 0.0003263161529659606\n",
      "Params: {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 5} Mean MSE: 0.000489790041431864\n",
      "Params: {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 10} Mean MSE: 0.0022022080668891\n",
      "Params: {'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 2} Mean MSE: 0.0002740591895484043\n",
      "Params: {'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 5} Mean MSE: 0.0004866925526767465\n",
      "Params: {'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 10} Mean MSE: 0.0022153558554459524\n",
      "Params: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 2} Mean MSE: 0.0003263161529659653\n",
      "Params: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 5} Mean MSE: 0.0004897900414318642\n",
      "Params: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 10} Mean MSE: 0.002202208066889094\n",
      "Params: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 2} Mean MSE: 0.00032631615296596205\n",
      "Params: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 5} Mean MSE: 0.0004897900414318663\n",
      "Params: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 10} Mean MSE: 0.002202208066889083\n",
      "\n",
      "Best Parameters for Random Forest: {'n_estimators': 150, 'max_depth': 10, 'min_samples_split': 2}\n",
      "Best Mean MSE for Random Forest: 0.00027088243348408897\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from itertools import product\n",
    "\n",
    "# Example parameter grid\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Define start_year, end_year, target_column for your expanding window CV\n",
    "start_year = 2010  # e.g., 2000\n",
    "end_year = 2016     # e.g., 2020\n",
    "target_column = 'target_TOT_POPULATION'  # replace with your target column\n",
    "\n",
    "# df should contain columns: 'Year', 'GEO_ID', target_column, and features\n",
    "# You must ensure df is prepared prior to running this code.\n",
    "# For example:\n",
    "# df = pd.DataFrame(...)\n",
    "# Make sure it has a 'Year' column, a 'GEO_ID' column, and a target column.\n",
    "\n",
    "best_score = np.inf\n",
    "best_params = None\n",
    "\n",
    "results_list = []\n",
    "\n",
    "# Iterate over all combinations of parameters\n",
    "for (n_est, max_d, min_split) in product(rf_param_grid['n_estimators'],\n",
    "                                                   rf_param_grid['max_depth'],\n",
    "                                                   rf_param_grid['min_samples_split']):\n",
    "    # Define the model with the given parameter combination\n",
    "    model = RandomForestRegressor(\n",
    "        random_state=42,\n",
    "        n_estimators=n_est,\n",
    "        max_depth=max_d,\n",
    "        min_samples_split=min_split,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Perform expanding window CV\n",
    "    cv_results = expanding_window_cv(df, start_year, end_year, target_column, model)\n",
    "\n",
    "    # Calculate mean MSE across all validation segments\n",
    "    mean_mse = cv_results['MSE'].mean()\n",
    "    \n",
    "    # Store results\n",
    "    params = {\n",
    "        'n_estimators': n_est,\n",
    "        'max_depth': max_d,\n",
    "        'min_samples_split': min_split,\n",
    "    }\n",
    "    results_list.append({\n",
    "        'params': params,\n",
    "        'mean_MSE': mean_mse\n",
    "    })\n",
    "    \n",
    "    # Print out the parameters and corresponding mean MSE\n",
    "    print(\"Params:\", params, \"Mean MSE:\", mean_mse)\n",
    "    \n",
    "    # Update best score and params if current is better\n",
    "    if mean_mse < best_score:\n",
    "        best_score = mean_mse\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest Parameters for Random Forest:\", best_params)\n",
    "print(\"Best Mean MSE for Random Forest:\", best_score)\n",
    "\n",
    "# Convert results to a DataFrame for inspection if you want\n",
    "final_results_df = pd.DataFrame(results_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs hyperparameter tuning for an XGBoost model using a grid search over specified parameter combinations, evaluates each configuration with expanding window cross-validation, and identifies the best parameters based on the lowest mean MSE across validation segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.025} Mean MSE: 1.0544733330298317\n",
      "Params: {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.05} Mean MSE: 0.007315448263825889\n",
      "Params: {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.1} Mean MSE: 0.0004596471463860209\n",
      "Params: {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.025} Mean MSE: 1.0544733330298317\n",
      "Params: {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.05} Mean MSE: 0.00731537427906152\n",
      "Params: {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1} Mean MSE: 0.00044858774979759426\n",
      "Params: {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.025} Mean MSE: 1.0544733330298317\n",
      "Params: {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.05} Mean MSE: 0.00731537427906152\n",
      "Params: {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1} Mean MSE: 0.0004667398076207074\n",
      "Params: {'n_estimators': 150, 'max_depth': 4, 'learning_rate': 0.025} Mean MSE: 0.0906562705548671\n",
      "Params: {'n_estimators': 150, 'max_depth': 4, 'learning_rate': 0.05} Mean MSE: 0.00027575376460853735\n",
      "Params: {'n_estimators': 150, 'max_depth': 4, 'learning_rate': 0.1} Mean MSE: 0.00045410828073918743\n",
      "Params: {'n_estimators': 150, 'max_depth': 5, 'learning_rate': 0.025} Mean MSE: 0.0906562705548671\n",
      "Params: {'n_estimators': 150, 'max_depth': 5, 'learning_rate': 0.05} Mean MSE: 0.00027555870769539477\n",
      "Params: {'n_estimators': 150, 'max_depth': 5, 'learning_rate': 0.1} Mean MSE: 0.000455921758884255\n",
      "Params: {'n_estimators': 150, 'max_depth': 6, 'learning_rate': 0.025} Mean MSE: 0.0906562705548671\n",
      "Params: {'n_estimators': 150, 'max_depth': 6, 'learning_rate': 0.05} Mean MSE: 0.0002762144121462271\n",
      "Params: {'n_estimators': 150, 'max_depth': 6, 'learning_rate': 0.1} Mean MSE: 0.0004703649110538626\n",
      "Params: {'n_estimators': 200, 'max_depth': 4, 'learning_rate': 0.025} Mean MSE: 0.008280917259490596\n",
      "Params: {'n_estimators': 200, 'max_depth': 4, 'learning_rate': 0.05} Mean MSE: 0.0001966644503309743\n",
      "Params: {'n_estimators': 200, 'max_depth': 4, 'learning_rate': 0.1} Mean MSE: 0.0004577548705175106\n",
      "Params: {'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.025} Mean MSE: 0.008280917259490596\n",
      "Params: {'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.05} Mean MSE: 0.00019573205744367173\n",
      "Params: {'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.1} Mean MSE: 0.00045838940965022755\n",
      "Params: {'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.025} Mean MSE: 0.008280917259490596\n",
      "Params: {'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.05} Mean MSE: 0.000196860273512103\n",
      "Params: {'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.1} Mean MSE: 0.00047328983096213883\n",
      "Params: {'n_estimators': 250, 'max_depth': 4, 'learning_rate': 0.025} Mean MSE: 0.0009580045074250768\n",
      "Params: {'n_estimators': 250, 'max_depth': 4, 'learning_rate': 0.05} Mean MSE: 0.0001932319148692646\n",
      "Params: {'n_estimators': 250, 'max_depth': 4, 'learning_rate': 0.1} Mean MSE: 0.0004639750355451874\n",
      "Params: {'n_estimators': 250, 'max_depth': 5, 'learning_rate': 0.025} Mean MSE: 0.0009576929597735345\n",
      "Params: {'n_estimators': 250, 'max_depth': 5, 'learning_rate': 0.05} Mean MSE: 0.00019312622438113247\n",
      "Params: {'n_estimators': 250, 'max_depth': 5, 'learning_rate': 0.1} Mean MSE: 0.00046109283475683426\n",
      "Params: {'n_estimators': 250, 'max_depth': 6, 'learning_rate': 0.025} Mean MSE: 0.000957378495112239\n",
      "Params: {'n_estimators': 250, 'max_depth': 6, 'learning_rate': 0.05} Mean MSE: 0.00019393638722295405\n",
      "Params: {'n_estimators': 250, 'max_depth': 6, 'learning_rate': 0.1} Mean MSE: 0.00047355560687414606\n",
      "\n",
      "Best Parameters for XGBoost: {'n_estimators': 250, 'max_depth': 5, 'learning_rate': 0.05}\n",
      "Best Mean MSE for XGBoost: 0.00019312622438113247\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from itertools import product\n",
    "\n",
    "# Example parameter grid for XGBoost\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 150, 200, 250],\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'learning_rate': [0.025, 0.05, 0.1]\n",
    "}\n",
    "\n",
    "# Define start_year, end_year, target_column for your expanding window CV\n",
    "start_year = 2010  \n",
    "end_year = 2016\n",
    "target_column = 'target_TOT_POPULATION'\n",
    "\n",
    "best_score = np.inf\n",
    "best_params = None\n",
    "\n",
    "results_list = []\n",
    "\n",
    "# Iterate over all combinations of parameters\n",
    "for (n_est, max_d, lr) in product(xgb_param_grid['n_estimators'],\n",
    "                                                xgb_param_grid['max_depth'],\n",
    "                                                xgb_param_grid['learning_rate']):\n",
    "\n",
    "    # Define the model with the given parameter combination\n",
    "    model = XGBRegressor(\n",
    "        random_state=42,\n",
    "        n_estimators=n_est,\n",
    "        max_depth=max_d,\n",
    "        learning_rate=lr,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Perform expanding window CV\n",
    "    cv_results = expanding_window_cv(df, start_year, end_year, target_column, model)\n",
    "\n",
    "    # Calculate mean MSE across all validation segments\n",
    "    mean_mse = cv_results['MSE'].mean()\n",
    "    \n",
    "    # Store results\n",
    "    params = {\n",
    "        'n_estimators': n_est,\n",
    "        'max_depth': max_d,\n",
    "        'learning_rate': lr\n",
    "    }\n",
    "    results_list.append({\n",
    "        'params': params,\n",
    "        'mean_MSE': mean_mse\n",
    "    })\n",
    "    \n",
    "    # Print out the parameters and corresponding mean MSE\n",
    "    print(\"Params:\", params, \"Mean MSE:\", mean_mse)\n",
    "    \n",
    "    # Update best score and params if current is better\n",
    "    if mean_mse < best_score:\n",
    "        best_score = mean_mse\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest Parameters for XGBoost:\", best_params)\n",
    "print(\"Best Mean MSE for XGBoost:\", best_score)\n",
    "\n",
    "# Convert results to a DataFrame for inspection if needed\n",
    "final_results_df = pd.DataFrame(results_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a function to perform hyperparameter tuning for regression models like Lasso or Ridge using expanding window cross-validation, iterating over all combinations of parameters from a grid to identify the configuration that minimizes mean MSE across validation segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grids for Lasso and Ridge\n",
    "lasso_param_grid = {\n",
    "    'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "}\n",
    "\n",
    "ridge_param_grid = {\n",
    "    'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Assuming expanding_window_cv is already defined as per your code\n",
    "# from your_existing_code import expanding_window_cv\n",
    "\n",
    "def tune_hyperparameters(df, start_year, end_year, target_column, model_class, param_grid, model_name_prefix):\n",
    "    \"\"\"\n",
    "    Tunes hyperparameters for a given regression model using expanding window cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - start_year: The starting year for cross-validation.\n",
    "    - end_year: The ending year for cross-validation.\n",
    "    - target_column: The name of the target column.\n",
    "    - model_class: The regression model class (e.g., Lasso, Ridge).\n",
    "    - param_grid: Dictionary containing the hyperparameters to tune.\n",
    "    - model_name_prefix: String prefix for the model name (e.g., 'Lasso').\n",
    "\n",
    "    Returns:\n",
    "    - best_params: The hyperparameters that achieved the lowest mean MSE.\n",
    "    - tuning_results: DataFrame containing MSE for each hyperparameter setting.\n",
    "    \"\"\"\n",
    "    tuning_results = []\n",
    "    best_score = np.inf\n",
    "    best_params = None\n",
    "\n",
    "    print(f\"Starting hyperparameter tuning for {model_name_prefix}...\")\n",
    "\n",
    "    # Iterate over all combinations of hyperparameters\n",
    "    for params in product(*param_grid.values()):\n",
    "        # Create a dictionary of current parameter settings\n",
    "        current_params = dict(zip(param_grid.keys(), params))\n",
    "        \n",
    "        # Initialize the model with current parameters\n",
    "        model = model_class(**current_params, random_state=42) if 'random_state' in model_class().get_params() else model_class(**current_params)\n",
    "        \n",
    "        # Perform expanding window CV\n",
    "        cv_results = expanding_window_cv(df, start_year, end_year, target_column, model)\n",
    "        \n",
    "        # Calculate mean MSE across all validation segments\n",
    "        mean_mse = cv_results['MSE'].mean()\n",
    "        \n",
    "        # Store results\n",
    "        tuning_results.append({\n",
    "            **current_params,\n",
    "            'Mean_MSE': mean_mse\n",
    "        })\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Params: {current_params}, Mean MSE: {mean_mse:.8f}\")\n",
    "        \n",
    "        # Update best parameters if current mean MSE is lower\n",
    "        if mean_mse < best_score:\n",
    "            best_score = mean_mse\n",
    "            best_params = current_params\n",
    "\n",
    "    tuning_results_df = pd.DataFrame(tuning_results)\n",
    "    \n",
    "    print(f\"Best parameters for {model_name_prefix}: {best_params} with Mean MSE: {best_score:.8f}\\n\")\n",
    "    \n",
    "    return best_params, tuning_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter tuning for Lasso Regression...\n",
      "Params: {'alpha': 1e-11}, Mean MSE: 0.00008498\n",
      "Params: {'alpha': 1e-05}, Mean MSE: 0.00007911\n",
      "Params: {'alpha': 0.0001}, Mean MSE: 0.00008294\n",
      "Params: {'alpha': 0.001}, Mean MSE: 0.00008616\n",
      "Params: {'alpha': 0.01}, Mean MSE: 0.00018487\n",
      "Params: {'alpha': 0.1}, Mean MSE: 0.01025032\n",
      "Params: {'alpha': 1.0}, Mean MSE: 0.53887931\n",
      "Best parameters for Lasso Regression: {'alpha': 1e-05} with Mean MSE: 0.00007911\n",
      "\n",
      "Starting hyperparameter tuning for Ridge Regression...\n",
      "Params: {'alpha': 1e-11}, Mean MSE: 0.00010936\n",
      "Params: {'alpha': 1e-05}, Mean MSE: 0.00008776\n",
      "Params: {'alpha': 0.0001}, Mean MSE: 0.00008376\n",
      "Params: {'alpha': 0.001}, Mean MSE: 0.00008370\n",
      "Params: {'alpha': 0.01}, Mean MSE: 0.00008515\n",
      "Params: {'alpha': 0.1}, Mean MSE: 0.00009102\n",
      "Params: {'alpha': 1.0}, Mean MSE: 0.00011641\n",
      "Best parameters for Ridge Regression: {'alpha': 0.001} with Mean MSE: 0.00008370\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Define the start and end years for cross-validation\n",
    "start_year = 2010\n",
    "end_year = 2016\n",
    "\n",
    "# Tune Lasso\n",
    "best_lasso_alpha, lasso_tuning_results = tune_hyperparameters(\n",
    "    df=df,\n",
    "    start_year=start_year,\n",
    "    end_year=end_year,\n",
    "    target_column=target_column,\n",
    "    model_class=Lasso,\n",
    "    param_grid=lasso_param_grid,\n",
    "    model_name_prefix='Lasso Regression'\n",
    ")\n",
    "\n",
    "# Tune Ridge\n",
    "best_ridge_alpha, ridge_tuning_results = tune_hyperparameters(\n",
    "    df=df,\n",
    "    start_year=start_year,\n",
    "    end_year=end_year,\n",
    "    target_column=target_column,\n",
    "    model_class=Ridge,\n",
    "    param_grid=ridge_param_grid,\n",
    "    model_name_prefix='Ridge Regression'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MSE for Linear Regression (alpha=0): 0.0001094\n"
     ]
    }
   ],
   "source": [
    "def compute_mean_mse_for_alpha_zero(df, start_year, end_year, target_column):\n",
    "    \"\"\"\n",
    "    Computes the mean MSE for Linear Regression (alpha=0) using expanding window cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - start_year: The starting year for cross-validation.\n",
    "    - end_year: The ending year for cross-validation.\n",
    "    - target_column: The name of the target column.\n",
    "\n",
    "    Returns:\n",
    "    - mean_mse: The mean MSE across all validation folds for Linear Regression.\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    # Initialize the Linear Regression model (alpha=0)\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Perform expanding window cross-validation\n",
    "    cv_results = expanding_window_cv(df, start_year, end_year, target_column, model)\n",
    "\n",
    "    # Calculate and return the mean MSE across all validation segments\n",
    "    mean_mse = cv_results['MSE'].mean()\n",
    "\n",
    "    print(f\"Mean MSE for Linear Regression (alpha=0): {mean_mse:.7f}\")\n",
    "    return mean_mse\n",
    "\n",
    "# Usage example\n",
    "mean_mse_linear_regression = compute_mean_mse_for_alpha_zero(\n",
    "    df=df,\n",
    "    start_year=start_year,\n",
    "    end_year=end_year,\n",
    "    target_column=target_column\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
